{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from enum import Enum\n",
    "import preprocessor as p\n",
    "import torch\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from src.model.BiLSTMClassifier import EnhancedBiLSTM\n",
    "from src.utils import label_to_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(Enum):\n",
    "    BiLSTM_GLOVE_100D = 3\n",
    "    BiLSTM_GLOVE_200D = 4\n",
    "    BiLSTM_FASTTEXT_300D = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_glove = np.load(\"../data/embeddings/embedding_matrix_glove_twitter_100.npy\")\n",
    "vocab = np.load(\"../data/embeddings/vocab.npy\", allow_pickle=True).item()\n",
    "\n",
    "vocab_size, embedding_dim_glove = embedding_matrix_glove.shape\n",
    "\n",
    "model_glove = EnhancedBiLSTM(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim_glove,\n",
    "    hidden_dim=256,\n",
    "    num_classes=20,\n",
    "    embedding_matrix=embedding_matrix_glove,\n",
    "    freeze_embeddings=True,\n",
    "    num_layers=2,\n",
    "    dropout=0.5\n",
    ")\n",
    "\n",
    "embedding_matrix_glove_200d = np.load(\"../data/embeddings/embedding_matrix_glove_twitter_200.npy\")\n",
    "embedding_dim_glove_200d = embedding_matrix_glove_200d.shape[1]\n",
    "\n",
    "model_glove_200d = EnhancedBiLSTM(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim_glove_200d,\n",
    "    hidden_dim=256,\n",
    "    num_classes=20,\n",
    "    embedding_matrix=embedding_matrix_glove_200d,\n",
    "    freeze_embeddings=True,\n",
    "    num_layers=2,\n",
    "    dropout=0.5\n",
    ")\n",
    "\n",
    "embedding_matrix_fasttext = np.load(\"../data/embeddings/embedding_matrix_crawl_subword_300.npy\")\n",
    "embedding_dim_fasttext = embedding_matrix_fasttext.shape[1]\n",
    "\n",
    "model_fasttext = EnhancedBiLSTM(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim_fasttext,\n",
    "    hidden_dim=128,\n",
    "    num_classes=20,\n",
    "    embedding_matrix=embedding_matrix_fasttext,\n",
    "    freeze_embeddings=True,\n",
    "    num_layers=2,\n",
    "    dropout=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text, vocab, max_len=50):\n",
    "    text = p.tokenize(text)\n",
    "    tokens = TweetTokenizer().tokenize(text.lower())\n",
    "    ids = [vocab.get(t, vocab[\"<UNK>\"]) for t in tokens]\n",
    "    if len(ids) < max_len:\n",
    "        ids = ids + [vocab[\"<PAD>\"]] * (max_len - len(ids))\n",
    "    else:\n",
    "        ids = ids[:max_len]\n",
    "    return ids\n",
    "\n",
    "def predict_emojis(text_ids, execute_model):\n",
    "    execute_model.eval()\n",
    "    with torch.no_grad():\n",
    "        X = torch.tensor([text_ids], dtype=torch.long)\n",
    "        logits = execute_model(X)\n",
    "        preds = logits.softmax(dim=1)[0].cpu().numpy()\n",
    "\n",
    "        preds = {label_to_emoji(i): float(preds[i]) for i in range(len(preds))}\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def infer(model: Model, text: str, top_k: int = 5):\n",
    "    execute_model = None\n",
    "\n",
    "    text_ids = encode(text, vocab)\n",
    "    result = None\n",
    "\n",
    "    if model == Model.BiLSTM_GLOVE_100D:\n",
    "        execute_model = model_glove\n",
    "        execute_model.load_state_dict(torch.load(f\"models/bilstm_glove_twitter_best.pth\", map_location=torch.device('cpu')))\n",
    "        result = predict_emojis(text_ids, execute_model)\n",
    "    elif model == Model.BiLSTM_GLOVE_200D:\n",
    "        execute_model = model_glove_200d\n",
    "        execute_model.load_state_dict(torch.load(f\"models/bilstm_glove_twitter_200d_best.pth\", map_location=torch.device('cpu')))\n",
    "        result = predict_emojis(text_ids, execute_model)\n",
    "    elif model == Model.BiLSTM_FASTTEXT_300D:\n",
    "        execute_model = model_fasttext\n",
    "        execute_model.load_state_dict(torch.load(f\"models/bilstm_fasttext_best.pth\", map_location=torch.device('cpu')))\n",
    "        result = predict_emojis(text_ids, execute_model)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model type\")\n",
    "    \n",
    "    sorted_emojis = sorted(result.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_emojis[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions using BiLSTM_FASTTEXT_300D:\n",
      "[('‚ù§', 0.4537658095359802), ('üòç', 0.19293206930160522), ('üíï', 0.061540428549051285), ('üíô', 0.0509660542011261), ('üòä', 0.04383263736963272)]\n"
     ]
    }
   ],
   "source": [
    "tweet = \"I love programming! #coding\"\n",
    "\n",
    "predictions = infer(Model.BiLSTM_FASTTEXT_300D, tweet)\n",
    "\n",
    "print(f\"Predictions using {Model.BiLSTM_FASTTEXT_300D.name}:\")\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
